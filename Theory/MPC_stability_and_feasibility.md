# Advanced Stability and Feasibility Analysis in Model Predictive Control Architectures

## 1. Introduction: The Finite Horizon Paradox

> [!WARNING]
> 1. WIP
> 2. The following notes are AI-made and include extra material for completeness. Parlo ita che so stanco, se stai leggendo questo vuol dire che non mi sono messo ancora a fixare questa pagina, spero non ci siano cagate e che le info extra che troverai ti possano far piacere. Baci e buon natale
> 3. latex su github online fa cagare, se vi danno fastidio le formule scritte dimmerda pullate in locale, aprite con vs code e scaricatevi un'estensione per le preview (tempo stimato 3 minuti) 


Model Predictive Control (MPC), historically referred to as Receding Horizon Control (RHC), represents a paradigm shift in the regulation of complex dynamical systems. Unlike classical control methodologies—such as Proportional-Integral-Derivative (PID) or Linear Quadratic Regulation (LQR)—which rely on pre-computed, static feedback laws, MPC is predicated on the real-time solution of a constrained Optimal Control Problem (OCP) at every sampling instant.  
 This distinct operational mechanic enables MPC to explicitly account for physical limitations, such as actuator saturation and safety boundaries, which are ubiquitous in industrial processes, automotive systems, and robotics.  
 However, the very feature that grants MPC its power—the finite prediction horizon—introduces a fundamental theoretical disconnect that has driven decades of research: the separation between the optimization horizon and the actual lifespan of the system. The core paradox of MPC lies in the approximation of an infinite-horizon control objective using a finite-horizon optimization. In an ideal theoretical setting, optimal control requires minimizing a cost function over an infinite horizon to guarantee that the state trajectory converges asymptotically to the origin. However, solving an infinite-horizon constrained optimization problem is generally computationally intractable for real-time embedded systems.  
 Consequently, the horizon is truncated to a finite integer $N$. This truncation severs the inherent link to closed-loop stability provided by the Bellman equation in infinite-horizon settings. An MPC controller might successfully plan a trajectory that minimizes cost over the next $N$ steps, yet fail to ensure stability or even feasibility at step $N+1$, potentially driving the system into a "blind alley" from which recovery is impossible. This report provides an exhaustive analysis of the theoretical mechanisms developed to bridge this gap. We explore the dual pillars of MPC theory: **Recursive Feasibility** and **Asymptotic Stability**. Drawing upon foundational lecture notes, seminal texts, and contemporary research, we dissect the mathematical architectures—such as terminal costs, invariant sets, and contractive constraints—that transform MPC from a heuristic strategy into a rigorous control methodology. Furthermore, we examine practical extensions for robust operation under uncertainty, including soft constraints, feasibility governors, and tube-based methods, ensuring the analysis remains grounded in the realities of engineering implementation.

 ### 1.1 The Operational Principle and the Feedback Gap
 The operation of MPC is defined by the Receding Horizon (RH) principle. At a specific time instant $k$, the controller measures the system state $x(k)$ and solves an open-loop optimization problem to find an optimal input sequence $u^\* = \{u^\* (k \vert k), u^\* (k+1 \vert k), \dots, u^\*(k+N-1 \vert k)\}$. Critically, only the first element of this sequence, $u^*(k|k)$, is applied to the plant. At the next time step $k+1$, the horizon shifts forward, and the optimization is repeated with fresh state data. This process implicitly defines a n**onlinear state feedback law** $u(k) = \kappa_{MPC}(x(k))$. The central challenge in stability analysis is that this feedback law is not available in closed form; it is the implicit result of a numerical optimization. Therefore, proving that the closed-loop system $x(k+1) = f(x(k), \kappa_{MPC}(x(k)))$ is stable requires demonstrating that the optimization procedure itself possesses properties analogous to a Lyapunov function. If the optimization problem is not carefully formulated—specifically regarding what happens at the end of the prediction horizon—the "greedy" nature of the finite-horizon controller can lead to instability, where the controller aggressively reduces short-term cost at the expense of long-term convergence.  
 ### 1.2 The Two Fundamental Questions
 The theoretical validation of any MPC scheme rests on answering two coupled questions  
 1. **Feasibility**: Is the finite horizon optimization problem solvable at every point in the state space? More specifically, does the existence of a solution at time $k=0$ guarantee the existence of a solution at all future times $k > 0$? 
     This property is known as Recursive Feasibility or Persistent Feasibility.
1. **Stability**: Does the sequence of control actions generated by the repeated optimization drive the system state to the origin (or target set) asymptotically? This requires ensuring that the closed-loop trajectories are bounded and converge to the equilibrium despite the finite "foresight" of the controller. 

These questions are inextricably linked. A controller that loses feasibility (crashes) is inherently unstable in a practical sense, while a controller that is unstable will eventually drive the state into regions where constraints may be violated, leading to infeasibility.
## 2. Mathematical Preliminaries and Problem Formulation
To rigorously analyze stability and feasibility, we must first establish the mathematical framework of the standard MPC problem. This formulation serves as the baseline for all subsequent stability proofs and modifications.
### 2.1 System Dynamics and Constraints
We consider a discrete-time, time-invariant dynamical system described by the difference equation:  

$$
x(k+1) = f(x(k), u(k))
$$

where $x(k) \in \mathbb{R}^n$ represents the system state and $u(k) \in \mathbb{R}^m$ represents the control input at time step $k$. The function $f: \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n$ describes the plant dynamics, which may be linear ($Ax+Bu$) or nonlinear. The system is subject to hard pointwise constraints on states and inputs, defined by the sets $\mathbb{X}$ and $\mathbb{U}$:  

$$
u(k) \in \mathbb{U} \subset \mathbb{R}^m
$$
$$
x(k) \in \mathbb{X} \subset \mathbb{R}^n
$$

Typically, $\mathbb{U}$ is a compact, convex polytope containing the origin (representing actuator saturation limits), and $\mathbb{X}$ is a closed, convex set representing safety limits or physical boundaries.
### 2.2 The Finite Horizon Optimal Control Problem (FHOCP)
At each sampling instant $k$, given the current state $x(k)$, the MPC controller solves the following constrained optimization problem, denoted as $\mathcal{P}_N(x(k))$:  

$$\min_{\mathbf{u}} J_N(x(k), \mathbf{u}) = V_f(x(k+N|k)) + \sum\_{i=0}^{N-1} \ell(x(k+i|k), u(k+i|k))
$$

Subject to:
  - **Dynamics**: $x(k+i+1|k) = f(x(k+i|k), u(k+i|k))$ for $i = 0, \dots, N-1$
  - **Initialization**: $x(k|k) = x(k)$ (the current measured state)
  - **Stage Constraints**: $u(k+i|k) \in \mathbb{U}$ and $x(k+i|k) \in \mathbb{X}$ for all $i$ in the horizon
  - **Terminal Constraint**: $x(k+N|k) \in \mathbb{X}_f \subseteq \mathbb{X}$

The objective function $J_N$ comprises two distinct components:
   - **Stage Cost $\ell(x,u)$**: A measure of performance deviation at each step. In standard tracking or regulation problems, this is typically a quadratic form $\ell(x,u) = x^T Q x + u^T R u$, where $Q \succeq 0$ and $R \succ 0$ are weighting matrices penalizing state error and control effort, respectively.
   - **Terminal Cost $V_f(x)$**: A penalty term applied to the final state of the prediction horizon. This term is crucial for approximating the "tail" of the infinite horizon cost that is neglected by the truncation to $N$ steps.
   - **Terminal Set $\mathbb{X}_f$**: A specific subset of the state space where the system must land at the end of the prediction horizon. This constraint ensures that the system is in a "safe" or "stabilizable" configuration at step $N$.
  
The solution to $\mathcal{P}_N(x(k))$ is the optimal control sequence $\mathbf{u}^\* = \{u^\*(0|k), \dots, u^\*(N-1|k)\}$ . The control input applied to the plant is $u(k) = u^*(0|k)$.
### 2.3 Definitions of Invariance and Stability
The analysis of MPC relies heavily on set-theoretic concepts, particularly invariance.
- **Positively Invariant Set**: A set $\Omega \subset \mathbb{R}^n$ is positively invariant for the closed-loop system $x(k+1) = f_{cl}(x(k))$ if for all $x(0) \in \Omega$, the trajectory $x(k)$ remains in $\Omega$ for all $k \ge 0$.
- **Control Invariant Set**: A set $\mathcal{C} \subset \mathbb{R}^n$ is control invariant for the system $x(k+1) = f(x(k), u(k))$ if for every $x \in \mathcal{C}$, there exists an admissible input $u \in \mathbb{U}$ such that $f(x, u) \in \mathcal{C}$. This definition implies that once the system enters a control invariant set, there exists a control strategy capable of keeping it there indefinitely while satisfying constraints.
- **Maximal Admissible Set ($O_\infty$)**: The largest set of initial states for which there exists a control law satisfying all constraints for all future time.

## 3. Feasibility Analysis: The Prerequisite for Control
Before stability can be addressed, the controller must be feasible. If the optimization problem $\mathcal{P}_N(x(k))$ has no solution, the controller effectively fails; it cannot generate a control signal that respects the physical limits of the system. This failure mode is unique to constrained optimal control and requires rigorous analysis.

### 3.1 Initial vs. Recursive Feasibility
We distinguish between feasibility at the initial time and feasibility at all future times. 
- **Initial Feasibility**: The initial state $x(0)$ must belong to the set of states $\mathcal{X}_N$ from which the terminal set $\mathbb{X}_f$ is reachable in $N$ steps. This set is formally defined as the $N$-step controllable set to $\mathbb{X}_f$.
- **Recursive Feasibility**: This property guarantees that if the problem is feasible at time $k$ (i.e., a valid trajectory exists), then the problem will necessarily be feasible at time $k+1$ under the applied control law. Without this guarantee, a system might start in a valid state but evolve into a state where no solution exists—a "dead end" in the state space.
### 3.2 The Mechanism of Recursive Feasibility: The Shifted Sequence 
The standard mathematical proof for recursive feasibility relies on the construction of a candidate solution for the next time step based on the solution from the current step. This is often referred to as the "shifting argument." Let the optimal input sequence at time $k$ be:  

$$
\mathbf{u}^\*(k) = \{ u^\*(k|k), u^\*(k+1|k), \dots, u^\*(k+N-1|k) \}
$$

This sequence generates a feasible state trajectory:

$$
\mathbf{x}^\*(k) = \{ x(k|k), x(k+1|k), \dots, x(k+N|k) \}
$$

where, by the terminal constraint, $x(k+N|k) \in \mathbb{X}_f$.At time $k+1$, the state of the system becomes $x(k+1) = x(k+1|k)$ (assuming no model mismatch). We need to find a sequence of $N$ inputs starting from $k+1$.We can reuse the remaining tail of the previous optimal sequence:

$$
\{ u^\*(k+1|k), \dots, u^\*(k+N-1|k) \}
$$

This covers the first $N-1$ steps of the new horizon. However, we are missing the input for the final step, $u(k+N|k+1)$.The state reached by the reused sequence is $x(k+N|k)$, which we know is inside the terminal set $\mathbb{X}\_f$.To ensure feasibility, we require that the terminal set $\mathbb{X}\_f$ is a Control Invariant Set. This means there exists a local control law (or simply a valid input) $\kappa\_f(x)$ such that for any $x \in \mathbb{X}\_f$, $f(x, \kappa\_f(x)) \in \mathbb{X}\_f$ and $\kappa\_f(x) \in \mathbb{U}$.Thus, we construct the candidate sequence $\tilde{\mathbf{u}}(k+1)$:

$$
\tilde{\mathbf{u}}(k+1) = \{ u^\*(k+1|k), \dots, u^\*(k+N-1|k), \kappa\_f(x(k+N|k)) \}
$$

Because the first $N-1$ parts are feasible (from the previous step) and the final step is feasible (by the invariance of $\mathbb{X}\_f$), the entire sequence is a valid solution to $\mathcal{P}\_N(x(k+1))$. The existence of a feasible solution implies that an optimal solution exists. Thus, feasibility is preserved recursively.
### 3.3 The "Determinedness Index" and Horizon Length
While terminal constraints guarantee feasibility, they can be conservative. If the prediction horizon $N$ is short, the set of states that can reach the terminal set $\mathbb{X}\_f$ (the feasible set $\mathcal{X}\_N$) may be small. As $N$ increases, the feasible set $\mathcal{X}\_N$ expands, eventually converging to the Maximal Control Invariant Set $\mathcal{C}\_\infty$.The Determinedness Index (denoted as $\bar{H}\_p$ or $d$) is formally defined as the smallest horizon length $N$ such that the $N$-step controllable set $\mathcal{X}\_N$ is equal to the maximal invariant set $\mathcal{C}\_\infty$.

$$
\bar{H}\_p = \min \{ N \in \mathbb{N} \mid \mathcal{K}\_N(\mathbb{X}\_f) = \mathcal{C}\_\infty \}
$$

The intuition here is analogous to driving a car on a road with obstacles. The determinedness index is the "minimum safe sight distance." If your headlights (horizon) illuminate the road further than the distance required to bring the car to a stop from full speed, extending the headlights further does not increase the set of safe initial states (i.e., you can't drive any faster or start from any more precarious positions). It can be highlighted that this index depends heavily on system dynamics; for a vehicle model, at low velocities, the index might be small (e.g., 2 steps), but it grows as velocity increases. Calculating this index offline helps engineers choose a prediction horizon $N \ge \bar{H}\_p$ that maximizes the operating region without wasting computational resources on excessively long horizons.
## 3.4 Invalidation via Bilevel Programming
While constructive proofs guarantee feasibility, it is also useful to determine if a given controller is not recursively feasible. Let's introduce a method using **Farkas' Lemma** and **bilevel programming** to search for "problematic" initial states. This approach poses an optimization problem that actively searches for a state $x_0$ which is initially feasible but leads to an infeasible state $x\_1$. 
- **The Search**: Maximize the violation of constraints at step $k+1$ subject to feasibility at step $k$
- **Certificate**: If the optimal value of this problem is zero (no violation possible), the controller is certified as recursively feasible. If positive, the optimizer returns a specific counter-example state that breaks the controller. This tool is invaluable for validating MPC designs where analytical proofs (like identifying $\mathbb{X}_f$) are difficult to construct explicitly.
## 4. Stability Analysis: Lyapunov Theory in MPC
Stability in MPC is established using Lyapunov stability theory. Unlike linear control, where eigenvalues of the closed-loop matrix $A\_{cl}$ dictate stability, MPC results in a nonlinear, time-varying feedback law. We must therefore find a scalar function $V(x)$ that decreases along the closed-loop trajectories. The most profound insight in MPC theory is that the Optimal Value Function $J\_N^\*(x)$ itself serves as this Lyapunov function.
### 4.1 The Lyapunov Descent Condition
To prove asymptotic stability, we must show that the optimal cost $J\_N^\*(x)$ decreases at each time step.

$$
J_N^\*(x(k+1)) - J\_N^\*(x(k)) \le -\ell(x(k), u(k))
$$

If this inequality holds, summing it over time implies that $\ell(x(k), u(k)) \to 0$, which (given standard observability assumptions on the stage cost) implies $x(k) \to 0$.  
**Detailed Proof via the Shifted Sequence**: Let $J\_N^\*(x(k))$ be the cost of the optimal sequence $\mathbf{u}^\*$ at time $k$: 

$$
J\_N^\*(x(k)) = \ell(x\_k, u^\*\_0) + \ell(x\_{k+1}, u^\*\_1) + \dots + \ell(x\_{k+N-1}, u^\*\_{N-1}) + V\_f(x\_{k+N})
$$

(Notation simplified: $x\_{k+i}$ implies predicted state).  
At time $k+1$, we consider the cost of the feasible candidate sequence $\tilde{\mathbf{u}}$ constructed in Section 3.2. Let this cost be $\tilde{J}\_N(x(k+1))$. By optimality, the actual optimal cost at $k+1$ must be less than or equal to the candidate cost: $J\_N^\*(x(k+1)) \le \tilde{J}\_N(x(k+1))$.The candidate sequence cost is:

$$
\tilde{J}\_N(x(k+1)) = \ell(x\_{k+1}, u^\*\_1) + \dots + \ell(x\_{k+N-1}, u^\*\_{N-1}) + \ell(x\_{k+N}, \kappa\_f(x\_{k+N})) + V\_f(x\_{k+N+1})
$$

Now, calculate the difference $\Delta J = \tilde{J}_N(x(k+1)) - J_N^*(x(k))$. Most terms in the summation cancel out (the overlapping parts of the trajectory). We are left with:
$$
\Delta J = -\ell(x_k, u^*_0) + \underbrace{ V_f(x_{k+N+1}) - V_f(x_{k+N}) + \ell(x_{k+N}, \kappa_f(x_{k+N})) }_{\text{Terminal Terms}}
$$
The term $-\ell(x_k, u^*_0)$ is the cost "consumed" by taking the first step. The bracketed terms represent the cost "added" by extending the horizon and moving the terminal state.  
For the Lyapunov condition to hold (i.e., $\Delta J \le -\ell(x_k, u^*_0)$), the bracketed term must be non-positive:
$$
V_f(f(x, \kappa_f(x))) - V_f(x) + \ell(x, \kappa_f(x)) \le 0, \quad \forall x \in \mathbb{X}_f
$$
This inequality is the ***Fundamental Stability Condition***. It essentially requires that the terminal cost $V_f(x)$ acts as a Control Lyapunov Function (CLF) for the terminal controller $\kappa_f(x)$ inside the terminal set $\mathbb{X}_f$. It ensures that the cost-to-go decreases faster than the stage cost accumulates in the infinite tail of the trajectory.
### 4.2 The "Standard" Stability Ingredients
Based on the derivation above, a standard stable MPC design requires three "Terminal Ingredients":
- **Terminal Set** $\mathbb{X}_f$: A positively invariant set under $\kappa_f(x)$.
- **Terminal Controller** $\kappa_f(x)$: A local feedback law (typically LQR) that stabilizes the system within $\mathbb{X}_f$ while satisfying input constraints
- **Terminal Cost** $V_f(x)$: A function (typically quadratic $x^TPx$) that satisfies the Lyapunov decrease inequality. Usually, $P$ is the solution to the Algebraic Riccati Equation (ARE) for the unconstrained system, which automatically satisfies the inequality with equality.
### 4.3 Classification of Stability Approaches
Different historical and modern MPC formulations satisfy this fundamental condition in different ways
#### 4.3.1 Terminal Equality Constraint (Kwon & Pearson, 1977)
This approach, detailed in 1 and 17, forces the state to reach the origin exactly at step $N$: $x(k+N|k) = 0$.
- **Mechanism**: Here, $\mathbb{X}_f = \{0\}$, $\kappa_f(x) = 0$, and $V_f(x) = 0$.
- **Proof**: The stability condition becomes $0 - 0 + \ell(0,0) \le 0$, which holds trivially.
- **Pros**: Theoretically simple; guarantees strong stability.
- **Cons**: Extremely restrictive. Requiring the state to hit zero in finite time often requires very large inputs, shrinking the feasible set significantly. It makes the optimization problem computationally difficult due to the stringent equality constraint.
#### 4.3.2 Terminal Set with Terminal Cost (Dual Mode)
This is the most popular modern formulation (often called "Dual Mode" MPC).
- **Mechanism**: The MPC steers the state into a neighborhood of the origin ($\mathbb{X}_f$). Once inside, the "virtual" controller switches to a local linear controller (LQR), and the terminal cost $V_f(x)$ captures the infinite cost of this linear phase
- **Pros**: Much larger domain of attraction than equality constraints. Shorter horizons can be used because the MPC only needs to get the system "close enough" to the origin, not exactly to it.
#### 4.3.3 No Terminal Constraints
Some formulations omit terminal constraints entirely to simplify implementation. Stability is then argued based on "sufficiently long" horizons. 
- **Intuition (Turnpike Property)**: For very long horizons, the optimal trajectory naturally spends most of its time near the origin to minimize the sum of stage costs. The contribution of the "tail" becomes negligible. 
- **Contractive Constraints**: An alternative approach, detailed in 17, enforces a "contractive" constraint $\|x(k+1)\| \le \alpha \|x(k)\|$ with $\alpha < 1$. This forces the state norm to decrease at every step, acting as a direct Lyapunov constraint. While robust, this can be overly conservative if the natural optimal path needs to temporarily increase the state norm (e.g., swinging up a pendulum) to achieve long-term minimization.
## 5. Robustness: Handling Uncertainty
Nominal stability proofs assume a perfect model ($x^+ = f(x,u)$). In reality, systems are subject to disturbances $w(k)$ and modeling errors: $x(k+1) = f(x(k), u(k)) + w(k)$. These uncertainties can break the "shifted sequence" logic, leading to infeasibility or instability.
### 5.1 Tube-Based MPC
Tube MPC is the prevailing method for robust constrained control.
- **Concept**: The controller maintains a "Nominal System" (a disturbance-free simulation) and forces the real system to stay close to it.
- **Decomposition**: The control input is split into $u(k) = \bar{u}(k) + K(x(k) - \bar{x}(k))$.$\bar{u}(k)$ is the optimal input for the nominal system $\bar{x}$.$K(x - \bar{x})$ is a fast feedback controller rejecting disturbances to keep the error $e = x - \bar{x}$ inside a bounded set $\mathbb{Z}$ (the "Tube")
- **Constraint Tightening**: The optimizer solves the nominal problem using tightened constraints $\mathbb{X}_{tight} = \mathbb{X} \ominus \mathbb{Z}$ and $\mathbb{U}_{tight} = \mathbb{U} \ominus K\mathbb{Z}$.
- **Guarantee**: If the nominal trajectory satisfies the tightened constraints, the real trajectory (which is $\bar{x} + e$) is guaranteed to satisfy the original constraints $\mathbb{X}$, preserving recursive feasibility robustly.
### 5.2 Min-Max MPC
This approach, mentioned in 2, solves for the worst-case disturbance scenario.
$$
\min_{\mathbf{u}} \max_{\mathbf{w}} J_N(x, \mathbf{u}, \mathbf{w})
$$
While theoretically rigorous, Min-Max MPC is computationally heavy because the number of possible disturbance scenarios grows exponentially with the horizon length. It is often overly conservative, optimizing performance for a "perfect storm" of disturbances that rarely occurs.
### 5.3 Disturbance Preview and Integrated Designs
Recent advances address the case where disturbances are not random but predictable (e.g., ocean waves for a ship, road grade for a car).
- **Augmented State**: The disturbance preview is augmented into the state vector.
- **Stability**: A new terminal cost/set is derived for the augmented system.
- **Intuition**: Instead of just rejecting the disturbance, the MPC can "surf" the disturbance if it helps minimize cost. This improves performance while maintaining ISS (Input-to-State Stability) by ensuring the controller anticipates the energy injection from the disturbance.
## 6. Practical Mechanisms for Feasibility and Stability
Beyond the core theoretical frameworks, several practical mechanisms are employed to ensure robust operation in industrial settings.
### 6.1 Soft Constraints and Exact Penalty Functions
In practice, disturbances might be larger than the "Tube" design allowance, or the initial state might be outside the feasible set due to an upset. A "Hard Constraint" MPC would become infeasible and crash.**Soft Constraints** relax state constraints using slack variables $\epsilon \ge 0$:
$$
x_{min} - \epsilon \le x \le x_{max} + \epsilon
$$
The cost function is modified: $J_{soft} = J + \rho \|\epsilon\|$.  
The choice of norm for the penalty $\rho \|\epsilon\|$ is critical for stability:
- **$L_2$ Norm (Quadratic)**: Adding $\rho \epsilon^2$ is numerically easy (keeps the problem a QP). However, it destroys the exact satisfaction of constraints. The solver will always allow a tiny violation to reduce the primary objective (performance vs. constraint trade-off). This effectively changes the constraint boundary, potentially leading to drift. 
- **$L_1$ / $L_\infty$ Norm (Exact Penalty)**: Adding $\rho |\epsilon|$ creates a non-smooth cost (or requires converting to linear programming form). However, it possesses the Exact Penalty Property: if a solution exists with $\epsilon=0$, the optimizer will find it. It only uses slack when absolutely physically necessary. This preserves the nominal stability properties and region of attraction while preventing solver failure during large disturbances.
### 6.2 Feasibility Governors
A Feasibility Governor (FG) is an add-on unit described in 11 that sits outside the MPC loop
- **Problem**: If the user changes the reference setpoint $r$ too abruptly, the target terminal set $\mathbb{X}_f(r)$ might become unreachable in $N$ steps, causing infeasibility.
- **Solution**: The FG filters the reference command. It computes a "safe" reference $v$ that is as close to $r$ as possible but guarantees that the new terminal set $\mathbb{X}_f(v)$ remains reachable
- **Benefit**: This decouples the feasibility problem from the tracking problem. The MPC always solves a feasible problem for the intermediate reference $v$, while the FG manages the long-term approach to $r$.
### 6.3 Pre-Stabilization
For open-loop unstable systems, the feasible region can be very small (narrow corridors in state space). If the MPC solution is calculated around the unstable open-loop dynamics, small numerical errors can result in infeasible predictions. Pre-stabilization 1 involves closing a loop with a linear controller $u = -Kx + c$ before designing the MPC. The MPC then optimizes the perturbation signal $c$.
- **Dynamics**: $x(k+1) = (A-BK)x(k) + Bc(k)$
- **Stability**: Since $(A-BK)$ is stable, the prediction model is stable. If the MPC horizon is truncated or the solver is stopped early (returning $c=0$), the system naturally decays to the origin rather than diverging. This drastically improves the numerical conditioning and the effective region of attraction.
## 7. New Frontiers: Economic and Data-Driven MPC
### 7.1 Economic MPC and Dissipativity
Traditional MPC minimizes "distance to setpoint." Economic MPC (EMPC) minimizes a generic economic cost (e.g., energy consumption, profit) that may not be minimal at the steady state.
- **Stability Challenge**: The standard Lyapunov argument ($J^*$ decreases) fails because the optimal operation might be a limit cycle (orbit) rather than a fixed point, or the cost might not be positive definite.
- **Rotated Stage Cost**: Stability is proven using Dissipativity Theory. The stage cost $\ell(x,u)$ is "rotated" using a storage function $\lambda(x)$:
  $$
  L(x,u) = \ell(x,u) + \lambda(x) - \lambda(f(x,u))
  $$
  If the system is strictly dissipative with respect to the optimal equilibrium, minimizing the raw economic cost is equivalent to minimizing this rotated cost, which is positive definite. This recovers the standard Lyapunov stability proofs for economic objectives.
### 7.2 Stability in Hybrid Architectures
In Hybrid Electric Vehicles (HEVs) and robotics, the system is hybrid (continuous dynamics + discrete modes like gears). 
- **Challenge**: The discrete switching (e.g., engine On/Off) makes the feasible set non-convex (union of disjoint sets).
- **Stability Solution**: These systems often use ***Hierarchical MPC***. A high-level planner (using coarse models or heuristics) determines the discrete sequence (mode schedule) over a long horizon to ensuring energy sustainability (a form of feasibility). The low-level MPC optimizes continuous inputs (torque) within that fixed mode sequence. Stability is ensured by constraining the low-level controller to track the energy trajectory of the high-level planner, effectively using the high-level plan as a dynamic terminal constraint.
## 8. Intuitive Analogies for Core Concepts
To make these abstract mathematical concepts concrete, we employ the analogy of driving a vehicle on a slippery road.  
|Concept | Mathematical Definition | Intuitive Analogy |
| ---| --- | --- |
| **Prediction Horizon ($N$)** | Optimization window length | The distance the driver's headlights illuminate the road |
| **Feasibility** | Existence of $u \in \mathbb{U}$ s.t. $x \in \mathbb{X}$ | The ability to steer and brake to stay on the road without crashing. |
| **Blind Alley (Infeasibility)**| $x(k)$ feasible $\nRightarrow x(k+1)$ feasible | Driving fast into a fog bank. You are safe now, but by the time you see the cliff edge at step $N+1$, it's too late to stop. |
| **Feasibility** | Existence of $u \in \mathbb{U}$ s.t. $x \in \mathbb{X}$ | The ability to steer and brake to stay on the road without crashing. |
| **Terminal Constraint ($\mathbb{X}_f$)** | $x(N) \in \mathbb{X}_{inv}$ | A rule: "You must plan your drive such that at the limit of your vision, the car is slow enough and positioned safely so you could stop indefinitely if needed." 
| **Recursive Feasibility** | Shifted sequence argument | Because you planned to be "safe" at the end of your vision, when you move forward, you just execute that safety maneuver. You never enter a situation where you rely on luck.
|**Determinedness Index** | Smallest $N$ for $\mathcal{K}_N = \mathcal{C}_\infty$ | The minimal braking distance. Seeing 10km ahead is no safer than seeing 200m ahead if 200m is all you need to stop from top speed. | Soft Constraints | Slack variables $\epsilon$ | Driving on the shoulder. It's technically "off-road" (violation), but permissible in an emergency to avoid a head-on collision (infeasibility).
|**Tube MPC** | Tightened constraints | Driving in the *center* of the lane. You leave a margin so that if a gust of wind (disturbance) hits you, you don't drift off the road.

## 9. Conclusion
The transition of Model Predictive Control from a heuristic industrial solution to a rigorously proven control methodology rests on the careful handling of **feasibility** and **stability**. The analysis reveals that the finite horizon optimization, while powerful, introduces a "feedback gap" that must be closed with explicit mathematical guarantees.  
The standard solution involves the "Stability Triad":
1. **Terminal Constraint**: Ensures the system reaches a stabilizable region.
2. **Terminal Cost**: Captures the "infinite tail" of the energy, correcting the shortsightedness of the horizon.
3. **Local Controller**: Provides the invariance property required to prove recursive feasibility.  
   
While the classic Kwon & Pearson approach (Terminal Equality) provides the strongest theoretical guarantees, it is practically limited by the difficulty of reaching the origin exactly in finite time. Modern approaches favor **Terminal Sets** (Dual Mode MPC) and **Tube-based Robust MPC**, which offer a better balance between region of attraction, computational complexity, and robustness.  
Furthermore, the emergence of **Soft Constraints** (with Exact Penalty functions) and **Feasibility Governors** reflects the practical necessity of prioritizing continued operation over strict adherence to constraints in the face of unexpected disturbances. These tools ensure that the controller degrades gracefully rather than failing catastrophically.  
Ultimately, the rigorous design of an MPC controller is an exercise in ensuring **Recursive Feasibility**: guaranteeing that every action taken today preserves the ability to find a valid solution tomorrow. Without this, optimization is merely a sophisticated way to crash; with it, MPC becomes a robust, high-performance strategy capable of pushing systems to their physical limits safely.
### Table 1: Comparative Analysis of MPC Stability Architectures
| Architecture | Terminal Constraint | Terminal Cost | Primary Advantage | Primary Disadvantage | 
|---|---|---|---|---|
**Zero Terminal Constraint** | None | Quadratic ($P_{LQR}$) | Computational Simplicity | Stability only guaranteed for very large $N$; difficult to tune. | 
| **Equality Constraint** | $x(N) = 0$ | None required| Simple theoretical proof | Very small Region of Attraction; requires high control effort | 
| **Dual Mode (Standard)** | $x(N) \in \mathbb{X}_f$ | $V_f(x)$ (Lyapunov) | Best balance of feasibility and stability | Requires computation of invariant sets ($\mathbb{X}_f$) offline | 
| **Contractive MPC** | $\|x_{k+1}\| \le \alpha \|x$ | Standard | Robust stability without terminal sets | Can be overly conservative; may prevent complex maneuvers | 
| **Tube MPC** | $x(N) \in \mathbb{X}_f$ (Tightened)|Standard | Robustness to bounded disturbances | Reduces feasible workspace due to margins (conservatism)



### Table 2: Key Mathematical Definitions and Their Significance
| Term | Definition | Role in MPC Theory |
|---|---|---|
| **Recursive Feasibility** | $x_k \in \mathcal{X}_N \implies x_{k+1} \in \mathcal{X}_N$ | Ensures the controller never encounters an unsolvable problem in the future
| **Control Invariant Set** | $\forall x \in \Omega, \exists u: f(x,u) \in \Omega$ | The mathematical definition of a "Safe Harbor" for terminal constraints 
| **CLF (Control Lyapunov Function)** | $V(f(x,u)) - V(x) < 0$ | The condition the Terminal Cost must satisfy to ensure the "tail" cost decreases 
| **Determinedness Index** | Min $N$ s.t. $\mathcal{K}_N = \mathcal{O}_\infty$ | The theoretical lower bound for the horizon length to maximize the controllable region
|**Exact Penalty Function** | $L_1$ norm on slack variables | Allows soft constraints that do not distort the trajectory when strictly feasible


